<!DOCTYPE html>
<html>
    <head>
        <link href="https://fonts.googleapis.com/css2?family=Exo&display=swap" rel="stylesheet">
        <meta charset="UTF-8">
        <meta name="author" content="nhomble">
        <meta name="description" content="nhomble's feed">
        <link rel="apple-touch-icon" sizes="180x180" href="resources/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="resources/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="resources/favicon-16x16.png">
        <link rel="manifest" href="resources/site.webmanifest">
        <title>fdmi</title>
    </head>
    <style>
        * {
            background: #F2F8FF;
            font-family: 'Exo', sans-serif;
            list-style-type: none;
        }
        li {
            padding-top: 1em
        }
    </style>
	<body>
	    <h1>feed mirror</h1>
	    <p>06/18 09:09:41 pm</p>
	    
	    <h4>cs.SE updates on arXiv.org</h4>
		<ul>
			<li><a href="https://arxiv.org/abs/2406.10263">A Lightweight Framework for Adaptive Retrieval In Code Completion With Critique Model</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10279">We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10300">Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10305">Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10317">Trusting code in the wild: Exploring contributor reputation measures to review dependencies in the Rust ecosystem</a></li>
		</ul>
		
	    <h4>cs.DL updates on arXiv.org</h4>
		<ul>
			<li><a href="https://arxiv.org/abs/2406.10535">Evaluating Open Access Advantages for Citations and Altmetrics (2011-21): A Dynamic and Evolving Relationship</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.11583">Where there's a will there's a way: ChatGPT is used more for science in countries where it is prohibited</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10274">Using General Large Language Models to Classify Mathematical Documents</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10541">Automating the Identification of High-Value Datasets in Open Government Data Portals</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.00008">KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery</a></li>
		</ul>
		
	    <h4>cs.DC updates on arXiv.org</h4>
		<ul>
			<li><a href="https://arxiv.org/abs/2406.10362">A Comparison of the Performance of the Molecular Dynamics Simulation Package GROMACS Implemented in the SYCL and CUDA Programming Models</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10474">Federated Neural Radiance Field for Distributed Intelligence</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10511">High-Performance Hardware Accelerator with Medium Granularity Dataflow for SpTRSV</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10707">DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models</a></li>
		
			<li><a href="https://arxiv.org/abs/2406.10728">Breaking the Memory Wall: A Study of I/O Patterns and GPU Memory Utilization for Hybrid CPU-GPU Offloaded Optimizers</a></li>
		</ul>
		
		<p><a href="https://github.com/nhomble/fdmi">src</a></p>
	</body>
</html>